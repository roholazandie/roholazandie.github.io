<!DOCTYPE html>
<!-- saved from url=(0044)https://semantic-ui.com/examples/sticky.html -->
<html class="gr__semantic-ui_com">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Standard Meta -->
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <!-- Site Properties -->
  <title>Sticky Example - Semantic</title>

  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/reset.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/site.css">

  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/container.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/grid.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/header.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/image.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/menu.css">

  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/divider.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/list.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/segment.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/dropdown.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/icon.css">
  <link rel="stylesheet" type="text/css" href="./Semantic-UI-CSS-master/components/transition.css">



<!-- Code snippet stylesheet-->
<link rel="stylesheet" href="./highlight/styles/default.css">
<script src="./highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<link class="codestyle" rel="stylesheet" href="./highlight/styles/zenburn.css">
<link class="codestyle" rel="stylesheet" href="./highlight/styles/dracula.css">
<link class="codestyle" rel="stylesheet" href="./highlight/styles/agate.css">





  <!-- MathJax Configuration -->
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>

  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script src="./Semantic-UI-CSS-master/components/transition.js"></script>
  <script src="./Semantic-UI-CSS-master/components/dropdown.js"></script>
  <script src="./Semantic-UI-CSS-master/components/visibility.js"></script>
  <script>
  $(document)
    .ready(function() {

      // fix main menu to page on passing
      $('.main.menu').visibility({
        type: 'fixed'
      });
      $('.overlay').visibility({
        type: 'fixed',
        offset: 80
      });

      // lazy load images
      $('.image').visibility({
        type: 'image',
        transition: 'vertical flip in',
        duration: 500
      });

      // show dropdown on hover
      $('.main.menu  .ui.dropdown').dropdown({
        on: 'hover'
      });
    })
  ;
  </script>

  <style type="text/css">

  body {
    background-color: #FFFFFF;
  }
  .main.container {
    margin-top: 2em;
  }

  .main.menu {
    margin-top: 4em;
    border-radius: 0;
    border: none;
    box-shadow: none;
    transition:
      box-shadow 0.5s ease,
      padding 0.5s ease
    ;
  }
  .main.menu .item img.logo {
    margin-right: 1.5em;
  }

  .overlay {
    float: left;
    margin: 0em 3em 1em 0em;
  }
  .overlay .menu {
    position: relative;
    left: 0;
    transition: left 0.5s ease;
  }

  .main.menu.fixed {
    background-color: #FFFFFF;
    border: 1px solid #DDD;
    box-shadow: 0px 3px 5px rgba(0, 0, 0, 0.2);
  }
  .overlay.fixed .menu {
    left: 800px;
  }

  .text.container .left.floated.image {
    margin: 2em 2em 2em -4em;
  }
  .text.container .right.floated.image {
    margin: 2em -4em 2em 2em;
  }

  .ui.footer.segment {
    margin: 5em 0em 0em;
    padding: 5em 0em;
  }
  </style>

</head>
<body data-gr-c-s-loaded="true">

 
  <div class="ui borderless main menu">
    <div class="ui text container">
      <div href="#" class="header item">
        <img class="logo" src="./images/logo.png">
        Curious Mind
      </div>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Blog</a>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Articles</a>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="ui right floated dropdown item" tabindex="0">
        Dropdown <i class="dropdown icon"></i>
        <div class="menu" tabindex="-1">
          <div class="item">Link Item</div>
          <div class="item">Link Item</div>
          <div class="divider"></div>
          <div class="header">Header Item</div>
          <div class="item">
            <i class="dropdown icon"></i>
            Sub Menu
            <div class="menu">
              <div class="item">Link Item</div>
              <div class="item">Link Item</div>
            </div>
          </div>
          <div class="item">Link Item</div>
        </div>
      </a>
    </div>
  </div><div class="ui borderless main menu placeholder" style="display: none;">
    <div class="ui text container">
      <div href="#" class="header item">
        <img class="logo" src="./images/logo.png">
        Project Name
      </div>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Blog</a>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Articles</a>
      <a href="https://semantic-ui.com/examples/sticky.html#" class="ui right floated dropdown item" tabindex="0">
        Dropdown <i class="dropdown icon"></i>
        <div class="menu" tabindex="-1">
          <div class="item">Link Item</div>
          <div class="item">Link Item</div>
          <div class="divider"></div>
          <div class="header">Header Item</div>
          <div class="item">
            <i class="dropdown icon"></i>
            Sub Menu
            <div class="menu">
              <div class="item">Link Item</div>
              <div class="item">Link Item</div>
            </div>
          </div>
          <div class="item">Link Item</div>
        </div>
      </a>
    </div>
  </div>

  <div class="ui text container">
    <p>Despite the huge advertisement and rush over many technologies in Artificial Intelligence, the core idea behind most of them are just as easy to grasp as 
    elementary mathematics! So, as the first rule we should have never be intimidated by the results or complexity of implementations. My experience always tell me
    that even the most sophisticated ideas can be easily understood to everyone. </p>
    <p>Likewise the idea behind neural networks and deep learning is a optimization problem with "netowrk loss" as the function to be optimized and gradient(derivation) as the way to find the parameters. The first problem in calculating this derivative is the huge number of parameters for even small netowrks. This might seem like an algorithmic problem: Finding a smart way to calculate something with lowest possible number of steps. But before trying to optimize the calculation of optimiztion problem I should define the problem in the simplest form</p>
    <p>
    	First of all, consider the simple network below with only two layers:
    </p>

    <div style="width:70%; margin-left:auto; margin-right:auto; margin-bottom:17px; margin-top:17px;">
    	<img class="ui large center image" data-src="./images/nn.png">
	</div>
    	    


    <p>
    	The network has two inputs $X_1$ and $X_2$ one two outputs $A_1^2$ and $A_2^2$ and a loss function $L$. The network takes two inputs(as a vector) then multiply them in weights in the first layer (matrix $W^1$) to create the output vector $Y^1$. The a function named the activation function takes each $Y^1$ and outputs the $A^1$. This calculation repeats in the next layer until we have the output vector $A^2$. Thed loss function $L$ takes $A^2$ vector and the actual output vector(label) and outputs a scalar that shows the difference between the actual output and the predicted output of the network. The only way to know if netowrk weights and biases are good enough is to measure the the difference between their outputs and the actual output with the aid of loss function. 
    </p>
    <p>
    	Consider the vector $X$ to be fed to the network. The following operations will be done on the input:
    	$$X=\begin{pmatrix}X_1\\X_2\end{pmatrix}$$

    	$$W^1 =\begin{pmatrix}
w_{1,1}^1 & w_{1,2}^1 \\
w_{2,1}^1 & w_{2,2}^1\end{pmatrix}$$

  	$$W^2 =\begin{pmatrix}
w_{1,1}^2 & w_{1,2}^2 \\
w_{2,1}^2 & w_{2,2}^2\end{pmatrix}$$

$$Y_{2\times1}^1=W_{2\times2}^1 X_{2\times1}+b_{2\times1}^1$$

$$A_{2\times1}^1=\sigma(Y_{2\times1}^1)$$

$$Y_{2\times1}^2=W_{2\times2}^2 A_{2\times1}^1+b_{2\times1}^2$$

$$A_{2\times1}^2=\sigma(Y_{2\times1}^2)$$

$$L_{1\times1}=||T-A^2||^2$$
    </p>
<p>
	Up to now, we just demonstrate the neural netowrk creation and calculation. The next step is to calculate the derivative of $L$ (the fucntion that need to be optimized) to every parameter in the network. For derivation there are some options:
	<ul>
		<li>Numerical differentiation</li>
		<li>Symbolic differentiation</li>
		<li>Automatic differentiation</li>
	</ul>
	Usally, in computer the numerical solutions are easier to implement and far more efficient than symbolic computations. Actually there is a big field, numerical analysis, that deals with implementing math problems inside computers. Even though this is generally true, as we proceed, we can see that it's not a universal rule.
</p>
<p>
	numerical differentiation describes algorithms for estimating the derivative of a mathematical function or function subroutine using values of the function and perhaps other knowledge about the function. The simplest method is to use finite difference approximations. A simple two-point estimation is to compute the slope of a nearby secant line through the points $(x, f(x))$ and $(x+h, f(x+h))$. The slope of this line is:
	$$\frac{f(x+h)-f(x)}{h}$$
	This formula seems to be simple and practical but in reality when you want to actually compute them there are some consideration. when the function is calculated using floating point arithmetic is how small a value of h to choose. If chosen too small, the subtraction will yield a large rounding error. In fact all the finite difference formulae are <a href="https://en.wikipedia.org/wiki/Condition_number">ill-conditioned</a>. and due to cancellation will produce a value of zero if h is small enough. If too large, the calculation of the slope of the secant line will be more accurately calculated, but the estimate of the slope of the tangent by using the secant could be worse.
</p>
<p>
	On the other hand naive symbolic differentation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression. But careful symbolic diffrentation is the best solution to this problem. Automatic differentation is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision
</p>
<p>
	But how we apply the chain rule in an efficient way? First we should know that every mathematical expression no matter how complex can be represented as a <a href="http://colah.github.io/posts/2015-08-Backprop/">computational graph</a>. For example the expression:
	$$f(X_1, X_2) = sin(X_1)+X`_1 \times X_2$$
	can be represented as the following computaional graph.
</p>
    <div style="width:70%; margin-left:auto; margin-right:auto; margin-bottom:17px; margin-top:17px;">
    	<img class="ui large right image" data-src="./images/cg3.png">
    </div>

<p>
	each node is an operation and the edges represent data that flows between them. In this graph each middle node can be seen as an intermediate computaion.
	$$z=f(X_1, X_2)$$
	$$z=X_1X_2+sin(X_1)$$
	$$W_1=X_1X_2$$
	$$W_2 = sin(X_1)$$
	$$W_3 = W_1 + W_2$$
	$$z=W_3$$
</p>
<p>
	<div style="width:70%; margin-left:auto; margin-right:auto; margin-bottom:17px; margin-top:17px;">
    	<img class="ui large right image" data-src="./images/der1.png">
    </div>
</p>
<p>
	Now, we use the chain rule to calculate derivatives. 
	$$\frac{\partial W_1}{\partial X_1} = \frac{\partial W_1}{\partial X_1} \frac{\partial X_1}{\partial X_1}$$
	$$\frac{\partial W_2}{\partial X_1}= \frac{\partial W_2}{\partial X_1} \frac{\partial X_1}{\partial X_1} + \frac{\partial W_2}{\partial X_2} \frac{\partial X_2}{\partial X_1}$$

	$$\frac{\partial W_3}{\partial X_1} = \frac{\partial W_2}{\partial W_1} \frac{\partial W_1}{\partial X_1} + \frac{\partial W_3}{\partial W_2} \frac{\partial W_2}{\partial X_1}$$

</p>
<p>
	The problem with the above calculation is that it just calculate the derivatives with respect to $X_1$. So, we need to calculate the derivative of the root with respect to every variable and this is not efficient. We call this calculation <i>forward accumulation</i> because it forward the derivative to the root.
</p>
<p>
	But, there is another approach which is based on dynamic programming. In dynamic programming we try to save intermediate calculation to be used instead of another recursive call. In the image below we start from the root and try to calculate the derivative of the root(here $W_3$) with respect to each node.
</p>
<p>
	<div style="width:70%; margin-left:auto; margin-right:auto; margin-bottom:17px; margin-top:17px;">
    	<img class="ui large right image" data-src="./images/der2.png">
    </div>
</p>
<p>
	So, we have:
	$$\frac{\partial W_3}{\partial X_1} = \frac{\partial W_2}{\partial W_1} \frac{\partial W_2}{\partial X_1} + \frac{\partial W_2}{\partial W_1} \frac{\partial W_1}{\partial X_1}$$
</p>
<p>
	Now, without calculating anything new we can find out $\frac{\partial W_3}{\partial X_2}$:
	$$\frac{\partial W_3}{\partial X_2} = \frac{\partial W_2}{\partial X_2} \frac{\partial W_3}{\partial W_2}$$
</p>

<p>
	The more generalized formulation can be expressed as:
	$$\frac{\partial L}{\partial A_1^{N-1}} = \frac{\partial L}{\partial A_1^N} \frac{\partial A_1^N}{\partial A_1^{N-1}}+...+\frac{\partial L}{\partial A_m^N} \frac{\partial A_m^N}{\partial A_m^{N-1}}$$

	$$\frac{\partial L}{\partial A_1^{N-1}} = \sum_{i=1}^{m} \frac{\partial L}{\partial A_i^N} \frac{\partial A_i^N}{\partial A_i^{N-1}}$$
</p>
<p>
	So, we can easily calculate the derivative with respect to weights and biases:
	$$\frac{\partial L}{\partial W_{jk}^{N-1}} = \frac{\partial L}{\partial A_1^{N-1}} \frac{\partial A_1^{N-1}}{\partial W_{jk}^{N-1}}$$
	$$\frac{\partial L}{\partial b_{j}^{N-1}} = \frac{\partial L}{\partial A_1^{N-1}} \frac{\partial A_1^{N-1}}{\partial b_{j}^{N-1}}$$
</p>
<p>
	Now, we can go back to our network and try to find the derivatives. Because of the vector nature of inputs and outputs of each stage, sometimes we need gradients and jacobians. We start from the root and go back to leaves:
	$$\nabla_{A^2} L = \frac{\partial L}{\partial A^2}=2(A^2-T)_{2 \times 1} $$
	$$\nabla_{Y^2} L = \frac{\partial L}{\partial Y^2}= \frac{\partial A^2}{\partial Y^2} \frac{\partial L}{\partial A^2} = 2(A^2-T)_{2 \times 1} \odot \sigma'(Y^2)_{2 \times 1}$$

	$$\nabla_{A^1} L = {[D_{A^2}(Y^2)]}^T \times \nabla_{Y^2} L = {[W^2]}^T \times 2(A^2-T)_{2 \times 1} \odot \sigma'(Y^2)_{2 \times 1}$$
	$$\nabla_{Y^1} L = {[D_{A^1}(Y^1)]}^T \times \nabla_{A^1} L = \nabla_{A^1} L \odot \sigma'(Y^1)_{2 \times 1}$$
	$$\nabla_{W^2} L = {\nabla_{Y^2}} L \times {[A^1]}^T $$
	$$\nabla_{b^2} L = {\nabla_{Y^2}} L \odot {[1]}^T $$
	$$\nabla_{W^1} L = {\nabla_{Y^1}} L \times X $$
	$$\nabla_{b^1} L = {\nabla_{Y^1}} L \odot b^1 $$

</p>
<p>
	Before going on, I want to show you some results from matrix calculus. When we have two function $F$ and $G$ with the following definitons:
	$$F:R^r \longrightarrow	 R^s$$
	$$G:R^s \longrightarrow	 R^t$$

	We can define the jacobian of their composition function $G(F(X))$ as follows:
	$$(GoF)(X) = G(F(X))$$
	$$J_{GoF}(X) = J_G(F(X))J_F(X)$$
	And for the special case when the function $G$ is a scalar funciton we have:
	$$F:R^r \longrightarrow	 R^s$$
	$$G:R^s \longrightarrow	 R^1$$

	$$\nabla_{GoF}^T(X)_{1 \times r} = \nabla^T(F(X))_{1 \times s} J_F(X)_{s \times r}$$
	$$\nabla_{GoF}(X)_{r \times 1} = J_F^T(X)_{r \times s} \nabla(F(X))_{s \times 1} $$

	The last formula has been used in our derivation above.
</p>
<p>
	Generally, when we have n layers we can write it down like:
	$$X_i = \phi(W_iX_{i-1}+b_i)=\psi(X_{i-1})$$
	$$X_n = \psi_n(\psi_{n-1}(...(\psi_0(X_0))...))$$
	And we can calculate the derivative with respect to $W_0$ as:
	$$J_{W_0}\psi_n = J_{\psi_{n-1}}\psi_n J_{\psi_{n-2}}\psi_{n-1} ... J_{\psi_{1}}\psi_{2} J_{W_0}\psi_{1}$$
</p>
<p>
	Now, we can go on and show the implementation in Python. Here I try to replicate the calculations and also use <a href="https://github.com/HIPS/autograd">autograd</a> to verify my results
	<p>
	<pre>
	<code class="hljs python">
import autograd.numpy as np
from autograd import grad
from autograd.convenience_wrappers import elementwise_grad
from autograd import jacobian
from autograd.util import flatten


def affine(W, X, b):
    f, _ = flatten(np.dot(W, X) + b)
    return f

def sigmoid(X):
    return 1. / (1 + np.exp(-X))

def squared(X):
    return X**2

def derivative_squred(X):
    return 2*X

def derivative_sigmoid(X):
    return sigmoid(X)*(1-sigmoid(X))

def loss(Y, T):
    return np.linalg.norm(Y - T) ** 2

def objective(params):
    X = np.array([[3.0], [2.0]])
    W = params[0]
    b = params[1]
    Y = affine(W, X, b).reshape((2, 1))

    #A = sigmoid(Y)
    A = squared(Y)

    T = np.array([[0.0], [1.0]])
    return loss(A, T)

if __name__ == "__main__":
    W1 = np.array([[2.0, 1.0], [1.0, 2.0]])
    b1 = np.array([[1.0], [2.0]])
    objective_grad = grad(objective)
    result = objective_grad([W1, b1])
    print(result)

    ###################################
    X = np.array([[3.0], [2.0]])
    W = W1
    b = b1
    Y = affine(W, X, b).reshape((2, 1))
    A = squared(Y)
    T = np.array([[0.0], [1.0]])


    grad_L_A = 2*(A-T)
    grad_A_Y = 2*Y
    grad_L_Y = grad_L_A * grad_A_Y
    print(grad_L_Y)

    grad_Y_W = X
    grad_L_W = np.dot(grad_L_Y, grad_Y_W.T)
    print(grad_L_W)

    ################################
    X = np.array([[3.0], [2.0]])
    W = W1
    b = b1
    Y = affine(W, X, b).reshape((2, 1))
    A = squared(Y)
    T = np.array([[0.0], [1.0]])

    grad_L_A = 2 * (A - T)
    grad_A_Y = 2 * Y
    grad_L_Y = grad_L_A * grad_A_Y
    print(grad_L_Y)

    grad_Y_W = X
    grad_L_W = np.dot(grad_L_Y, grad_Y_W.T)
    print(grad_L_W)
	</code>
	</pre>
		
	</p>
</p>
	
    <div class="overlay">
      <div class="ui labeled icon vertical menu">
        <a class="item"><i class="twitter icon"></i> Tweet</a>
        <a class="item"><i class="facebook icon"></i> Share</a>
        <a class="item"><i class="mail icon"></i> E-mail</a>
      </div>
    </div><div class="overlay placeholder" style="display: none;">
      <div class="ui labeled icon vertical menu">
        <a class="item"><i class="twitter icon"></i> Tweet</a>
        <a class="item"><i class="facebook icon"></i> Share</a>
        <a class="item"><i class="mail icon"></i> E-mail</a>
      </div>
    </div>
  
  </div>

  <div class="ui inverted vertical footer segment">
    <div class="ui center aligned container">
      <div class="ui stackable inverted divided grid">
        <div class="three wide column">
          <h4 class="ui inverted header">Group 1</h4>
          <div class="ui inverted link list">
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link One</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Two</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Three</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Four</a>
          </div>
        </div>
        <div class="three wide column">
          <h4 class="ui inverted header">Group 2</h4>
          <div class="ui inverted link list">
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link One</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Two</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Three</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Four</a>
          </div>
        </div>
        <div class="three wide column">
          <h4 class="ui inverted header">Group 3</h4>
          <div class="ui inverted link list">
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link One</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Two</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Three</a>
            <a href="https://semantic-ui.com/examples/sticky.html#" class="item">Link Four</a>
          </div>
        </div>
        <div class="seven wide column">
          <h4 class="ui inverted header">Footer Header</h4>
          <p>Extra space for a call to action inside the footer that could help re-engage users.</p>
        </div>
      </div>
      <div class="ui inverted section divider"></div>
      <img src="./images/logo.png" class="ui centered mini image">
      <div class="ui horizontal inverted small divided link list">
        <a class="item" href="https://semantic-ui.com/examples/sticky.html#">Site Map</a>
        <a class="item" href="https://semantic-ui.com/examples/sticky.html#">Contact Us</a>
        <a class="item" href="https://semantic-ui.com/examples/sticky.html#">Terms and Conditions</a>
        <a class="item" href="https://semantic-ui.com/examples/sticky.html#">Privacy Policy</a>
      </div>
    </div>
  </div>



<div style="clear: both;"></div></body></html>